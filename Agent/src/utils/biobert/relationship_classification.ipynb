{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import modeling\n",
    "import pandas\n",
    "import tokenization\n",
    "import os\n",
    "import collections\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids, labels, num_labels, use_one_hot_embeddings):\n",
    "    model = modeling.BertModel(config=bert_config, is_training=is_training, input_ids=input_ids,\n",
    "                               input_mask=input_mask, token_type_ids=segment_ids, use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "    output_layer = model.get_pooled_output()\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "    output_weights = tf.get_variable(\"output_weights\", [num_labels, hidden_size],\n",
    "                                     initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "    output_bias = tf.get_variable(\"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "        return (loss, per_example_loss, logits, probabilities)\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate, \n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        is_real_example = None\n",
    "        if \"is_real_example\" in features:\n",
    "            is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "        else:\n",
    "            is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, use_one_hot_embeddings)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "                scaffold_fn = tpu_scaffold\n",
    "            else:\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                \n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "      \n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            train_op = optimization.create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                scaffold_fn=scaffold_fn)\n",
    "            \n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "            def metric_fn(per_example_loss, label_ids, logits, is_real_example):\n",
    "                predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                accuracy = tf.metrics.accuracy(\n",
    "                    labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "                loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "                return {\n",
    "                    \"eval_accuracy\": accuracy,\n",
    "                    \"eval_loss\": loss,\n",
    "                }\n",
    "            eval_metrics = (metric_fn,[per_example_loss, label_ids, logits, is_real_example])\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metrics=eval_metrics,\n",
    "                scaffold_fn=scaffold_fn)\n",
    "        else:\n",
    "            output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\"probabilities\": probabilities},\n",
    "                scaffold_fn=scaffold_fn)\n",
    "        return output_spec\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "    \n",
    "class DataProcessor(object):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_test_examples(self, data_dir):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_example(self, data_eg):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_labels(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        with tf.gfile.Open(input_file, \"r\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "class BioBERTChemprotProcessor(DataProcessor):\n",
    "    def get_train_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "    def get_test_examples(self, data_dir):\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\")\n",
    "    \n",
    "    def get_test_example(self, data_eg):\n",
    "        line = [['index', 'sentence', 'label']]\n",
    "        line.append(['0', data_eg])\n",
    "        return self._create_examples(line, \"test\")\n",
    "    \n",
    "    def get_labels(self):\n",
    "        return [\"cpr:3\", \"cpr:4\", \"cpr:5\", \"cpr:6\", \"cpr:9\", \"false\"]\n",
    "    \n",
    "    def _create_examples(self, lines, set_type):\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if set_type == \"test\" and i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            if set_type == \"test\":\n",
    "                text_a = tokenization.convert_to_unicode(line[1])\n",
    "                label = \"false\"\n",
    "            else:\n",
    "                text_a = tokenization.convert_to_unicode(line[0])\n",
    "                label = tokenization.convert_to_unicode(line[1])\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputFeatures(object):\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id, is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "        self.is_real_example = is_real_example\n",
    "    \n",
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "\n",
    "def convert_single_example(ex_index, example, label_list, max_seq_length,\n",
    "                           tokenizer):\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures( input_ids=[0] * max_seq_length, input_mask=[0] * max_seq_length,\n",
    "                             segment_ids=[0] * max_seq_length, label_id=0,is_real_example=False)\n",
    "    label_map = {}\n",
    "    for (i, label) in enumerate(label_list):\n",
    "        label_map[label] = i\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "    if tokens_b:\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "    \n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "        \n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    input_mask = [1] * len(input_ids)\n",
    "    \n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "        \n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    \n",
    "    label_id = label_map[example.label]\n",
    "    \n",
    "    feature = InputFeatures(input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids,\n",
    "                            label_id=label_id, is_real_example=True)\n",
    "    return feature\n",
    "\n",
    "def file_based_convert_examples_to_features(examples, label_list, max_seq_length, tokenizer, output_file):\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        feature = convert_single_example(ex_index, example, label_list, \n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"label_ids\"] = create_int_feature([feature.label_id])\n",
    "        features[\"is_real_example\"] = create_int_feature([int(feature.is_real_example)])\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_based_input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "    \n",
    "    def _decode_record(record, name_to_features):\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "        return example\n",
    "    \n",
    "    def input_fn(params):\n",
    "        batch_size = params[\"batch_size\"]\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "        return d\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationship_classification(data_line):\n",
    "    biobert_re_finetuned_vocab = os.path.join(os.path.join(os.getcwd(),\"re_outputs\"),\"vocab.txt\")\n",
    "    biobert_re_finetuned_config = os.path.join(os.path.join(os.getcwd(),\"re_outputs\"),\"bert_config.json\")\n",
    "    biobert_re_finetuned_ckpt = os.path.join(os.path.join(os.getcwd(),\"re_outputs\"),\"model.ckpt-389\")\n",
    "    output_directory = os.path.join(os.getcwd(),\"RE_try\")\n",
    "    # re_dir = os.path.join(os.path.join(os.path.join(os.path.join(os.getcwd(),\"datasets\"),\"RE\"),\"chemprot\"),\"scibert_preprocess\")\n",
    "    tokenization.validate_case_matches_checkpoint(False, biobert_re_finetuned_ckpt)\n",
    "\n",
    "    ###### Extract BERT Configuration file --- LINE 969 of run_re.py \n",
    "    bert_config = modeling.BertConfig.from_json_file(biobert_re_finetuned_config)\n",
    "    \n",
    "    processor = BioBERTChemprotProcessor()\n",
    "\n",
    "    ###### Note list of labels --- LINE 986 of run_re.py\n",
    "    label_list = processor.get_labels()\n",
    "\n",
    "    ###### Get Tokenizer --- LINE 988 of run_re.py\n",
    "    tokenizer = tokenization.FullTokenizer(vocab_file=biobert_re_finetuned_vocab, do_lower_case=False)\n",
    "    \n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    run_config = tf.contrib.tpu.RunConfig(cluster= None,\n",
    "                                      master = None,\n",
    "                                      model_dir=output_directory,\n",
    "                                      save_checkpoints_steps=1000,\n",
    "                                      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "                                          iterations_per_loop=1000,\n",
    "                                          num_shards=8,\n",
    "                                          per_host_input_for_training=is_per_host))\n",
    "    model_fn = model_fn_builder(bert_config=bert_config, \n",
    "                            num_labels=len(label_list), \n",
    "                            init_checkpoint=biobert_re_finetuned_ckpt,\n",
    "                            learning_rate=5e-5,\n",
    "                            num_train_steps=None,\n",
    "                            num_warmup_steps=None,\n",
    "                            use_tpu=False,\n",
    "                            use_one_hot_embeddings=False)\n",
    "\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(use_tpu=False, model_fn=model_fn, config=run_config, predict_batch_size=8)\n",
    "    predict_example = processor.get_test_example(data_line)\n",
    "    #  predict_examples = processor.get_test_examples(re_dir)\n",
    "    num_actual_predict_examples = len(predict_example)\n",
    "    predict_file = os.path.join(output_directory, \"predict.tf_record\")\n",
    "    file_based_convert_examples_to_features(predict_example, \n",
    "                                        label_list,\n",
    "                                        128, \n",
    "                                        tokenizer,\n",
    "                                        predict_file)\n",
    "    predict_input_fn = file_based_input_fn_builder(input_file=predict_file,\n",
    "                                               seq_length=128,\n",
    "                                               is_training=False,\n",
    "                                               drop_remainder=False)\n",
    "    result = estimator.predict(input_fn=predict_input_fn)\n",
    "    for (i, prediction) in enumerate(result):\n",
    "        probabilities = prediction[\"probabilities\"]\n",
    "        x = label_list[np.argmax(probabilities)]\n",
    "        if x==\"cpr:3\":\n",
    "            label_name = \"UPREGULATOR|ACTIVATOR|INDIRECT_UPREGULATOR\"\n",
    "        elif x==\"cpr:4\":\n",
    "            label_name = \"DOWNREGULATOR|INHIBITOR|INDIRECT_DOWNREGULATOR\"\n",
    "        elif x==\"cpr:5\":\n",
    "            label_name = \"AGONIST|AGONIST‐ACTIVATOR|AGONIST‐INHIBITOR\"\n",
    "        elif x==\"cpr:6\":\n",
    "            label_name = \"ANTAGONIST\"\n",
    "        elif x==\"cpr:9\":\n",
    "            label_name = \"SUBSTRATE|PRODUCT_OF|SUBSTRATE_PRODUCT_OF\"\n",
    "        else:\n",
    "            label_name = \"No RELATION\"\n",
    "        return(label_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
